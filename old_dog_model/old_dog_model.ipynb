{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96221ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simon Dudtschak\n",
    "# 20166103\n",
    "#\n",
    "# Based on: SOM-CNN by Ashraf Neisari\n",
    "# Available: https://github.com/Ashsari/spam_review_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7fee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from preprocessing import Preprocessing\n",
    "from Clustering_words_SOM import Clustering\n",
    "from Constructing_images_classification import Classify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from SRD_minisom import MiniSom\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import collections\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6fd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.randint(0,10000)\n",
    "np.random.seed(rand)\n",
    "random.seed(rand)\n",
    "\n",
    "globalTime = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517eee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRE-PROCESSING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "##                      PRE-PROCESSING                      ##\n",
    "##############################################################\n",
    "print(\"\\nPRE-PROCESSING\\n\")\n",
    "\n",
    "#importing the dataset\n",
    "#@EXTENSION\n",
    "dataset = pd.read_csv('final_mfrc_data.csv')\n",
    "reviews = dataset.iloc[:,0]\n",
    "\n",
    "#get labels and convert them to categorical format\n",
    "list_words = []\n",
    "for row in dataset[\"annotation\"]:\n",
    "    words = row.split(',')\n",
    "    word_counts = collections.Counter(words)\n",
    "    list_words.append(words)\n",
    "    flat_list = list(chain.from_iterable(list_words))\n",
    "unique_words = set(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a6b463c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get labels and convert them to categorical format\n",
    "y = dataset.iloc[:,3]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "label = to_categorical(y, num_classes = 6)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c834b42e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning\n"
     ]
    }
   ],
   "source": [
    "#cleaning the vocabulary and tokenizing the reviews\n",
    "print(\"Cleaning\")\n",
    "vocabulary, numWords, review_vocab, cleaned_reviews = Preprocessing.cleaning_reviews(reviews)\n",
    "\n",
    "# keeping only unique words     \n",
    "vocabulary = list(set(vocabulary))\n",
    "\n",
    "# Here we provide the pretrained word embedding dictionary of glove with the vector dimension of 300\n",
    "embedding_dim = 300\n",
    "filepath ='glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f75b495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "The percentage of the covered words in the pretrained dictionary:  0.8740046287216099\n",
      "number of not found words is:  3212\n"
     ]
    }
   ],
   "source": [
    "#Here we construct the embedding matrix of the fetched vocabulary from the reviews. \n",
    "#Each row of the matrix represents the corresponding word embedding vector\n",
    "print(\"Embedding\")\n",
    "embedding_matrix = Preprocessing.create_embedding_matrix(filepath, vocabulary, embedding_dim )\n",
    "\n",
    "#We fetch the list of the words that are not covered by the pretrained dictionary and later eliminate them from the vocabulary list  \n",
    "nonzero_elements, not_foundWords, not_foundIndex = Preprocessing.not_found_words(embedding_matrix, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18e1e256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling make all features(the vectors values) between 0 and 1\n",
    "print(\"Scaling\")\n",
    "X = embedding_matrix\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4356e1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25493, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51b9ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\simon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.6 GiB for an array with shape (61226, 25457) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26036\\2875251955.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#calculate the TF-IDF of the vocabularies from the cleaned reviews\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_TF_IDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissed_tfidfW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords_frequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Coding\\CMPE351\\cmpe351_group10\\old_dog_model\\preprocessing.py\u001b[0m in \u001b[0;36mwords_frequency\u001b[1;34m(cleaned_reviews, vocabulary)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hello friend!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mdense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mdenselist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\simon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    849\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m         \"\"\"\n\u001b[1;32m--> 851\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\simon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\simon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.6 GiB for an array with shape (61226, 25457) and data type float64"
     ]
    }
   ],
   "source": [
    "#calculate the TF-IDF of the vocabularies from the cleaned reviews \n",
    "df_TF_IDF, missed_tfidfW = Preprocessing.words_frequency(cleaned_reviews, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7993666",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##                      CLUSTERING                      ##\n",
    "##########################################################\n",
    "print(\"\\nCLUSTERING\\n\")\n",
    "\n",
    "\n",
    "#define the grid size for the SOM map by g.\n",
    "#@EXTENSION\n",
    "g = 50\n",
    "X_of_map = g\n",
    "y_of_map = g\n",
    "\n",
    "#the input length passed to the SOM is the embedding vectors dimension \n",
    "input_len = X.shape[1]\n",
    "\n",
    "#defining the neghborhood radius sigma as s and the learning rate\n",
    "s = 1; l = 1 # the radius\n",
    "\n",
    "#We create a SOM object and train it on the word vectors that were prepared in the pre-processing step\n",
    "som = MiniSom(x = X_of_map, y= y_of_map, input_len = input_len, sigma = s, learning_rate = l, random_seed = 8)\n",
    "#Randomly initialize the weights that are assigne to each grid cells\n",
    "som.random_weights_init(X)\n",
    "\n",
    "#trining the SOM\n",
    "print(\"SOM Training\")\n",
    "#@EXTENSION\n",
    "som.train_batch(data = X, num_iteration = 1000)\n",
    "\n",
    "\n",
    "\n",
    "#separating ham and spam reviews from in the dataset\n",
    "print(\"Seperate\")\n",
    "ham , spam =Clustering.separate_ham_spam(reviews, y)\n",
    "#getting vocabulary list that are used in ham and spam reviews\n",
    "\n",
    "print(\"Cleaning\")\n",
    "vocabulary_ham, numWords_ham, review_vocab_ham, cleaned_reviews_ham = Preprocessing.cleaning_reviews(ham)\n",
    "vocabulary_spam, numWords_spam, review_vocab_spam, cleaned_reviews_spam = Preprocessing.cleaning_reviews(spam)\n",
    "vocabulary_ham = list(set(vocabulary_ham))\n",
    "vocabulary_spam = list(set(vocabulary_spam))\n",
    "\n",
    "#creating a list of words that are common among the ham and spam reviews\n",
    "matches = list(set(vocabulary_spam) & set(vocabulary_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff64b2d0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Win Map\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'som' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26036\\2952178150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#fetching the winning nodes of the word vectors from the SOM grid map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Calculate Win Map\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmappings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwin_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'som' is not defined"
     ]
    }
   ],
   "source": [
    "#fetching the winning nodes of the word vectors from the SOM grid map\n",
    "print(\"Calculate Win Map\")\n",
    "mappings = som.win_map(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "##                      CLASSIFICATION                      ##\n",
    "##############################################################\n",
    "print(\"\\nCLASSIFICATION\\n\")\n",
    "\n",
    "#mappings = som.win_map(X)\n",
    "map_cells = Classify.words_win_cel(mappings, vocabulary) \n",
    "\n",
    "#constructing the review images based on the feature word density\n",
    "print(\"Create Density Image\")\n",
    "dense_img = Classify.create_dens_img(review_vocab, map_cells, vocabulary, not_foundWords, X_of_map, y_of_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing the review images based on the feature word frequency \n",
    "print(\"Create Frequency Image\")\n",
    "frequency_img= Classify.create_freq_img(review_vocab, map_cells, vocabulary, missed_tfidfW, not_foundWords, X_of_map, y_of_map, df_TF_IDF)    \n",
    "\n",
    "\n",
    "\n",
    "#reshaping the images to make them ready for the CNN\n",
    "review_images = Classify.reshape_img(dense_img, frequency_img) \n",
    "\n",
    "#splitting the data to training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(review_images ,label, test_size=0.2,random_state= 252)\n",
    "print(\"Create Model\")\n",
    "model = Classify.create_model(g, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the evaluation performance of the model\n",
    "loss, tr_accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(tr_accuracy)) # \n",
    "loss, te_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(te_accuracy)) # \n",
    "print('\\n')\n",
    "\n",
    "pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "predict_classes = np.argmax(pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis = 1)\n",
    "#print the classification report\n",
    "evaluation = classification_report(expected_classes, predict_classes)\n",
    "print(evaluation)\n",
    "\n",
    "#plotting ROC value of the result\n",
    "matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "ROC_value, fprx, tpry = Classify.ROC_Val(matrix)\n",
    "\n",
    "globalTime = time.perf_counter() - globalTime\n",
    "print(\"Total Time: \", globalTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
