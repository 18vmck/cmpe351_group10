{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb563b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simon Dudtschak\n",
    "# 20166103\n",
    "#\n",
    "# Based on: SOM-CNN by Ashraf Neisari\n",
    "# Available: https://github.com/Ashsari/spam_review_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794f4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from preprocessing import Preprocessing\n",
    "from Clustering_words_SOM import Clustering\n",
    "from Constructing_images_classification import Classify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from SRD_minisom import MiniSom\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import collections\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632cfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.randint(0,10000)\n",
    "np.random.seed(rand)\n",
    "random.seed(rand)\n",
    "\n",
    "globalTime = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1865e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRE-PROCESSING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "##                      PRE-PROCESSING                      ##\n",
    "##############################################################\n",
    "print(\"\\nPRE-PROCESSING\\n\")\n",
    "\n",
    "#importing the dataset\n",
    "#@EXTENSION\n",
    "dataset = pd.read_csv('final_mfrc_data.csv')\n",
    "reviews = dataset.iloc[:,0]\n",
    "\n",
    "#get labels and convert them to categorical format\n",
    "list_words = []\n",
    "for row in dataset[\"annotation\"]:\n",
    "    words = row.split(',')\n",
    "    word_counts = collections.Counter(words)\n",
    "    list_words.append(words)\n",
    "    flat_list = list(chain.from_iterable(list_words))\n",
    "unique_words = set(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b8c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels and convert them to categorical format\n",
    "y = dataset.iloc[:,3]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "label = to_categorical(y, num_classes = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fcafcee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning\n"
     ]
    }
   ],
   "source": [
    "#cleaning the vocabulary and tokenizing the reviews\n",
    "print(\"Cleaning\")\n",
    "vocabulary, numWords, review_vocab, cleaned_reviews = Preprocessing.cleaning_reviews(reviews)\n",
    "\n",
    "# keeping only unique words     \n",
    "vocabulary = list(set(vocabulary))\n",
    "\n",
    "# Here we provide the pretrained word embedding dictionary of glove with the vector dimension of 300\n",
    "embedding_dim = 300\n",
    "filepath ='glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1863bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\simon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#calculate the TF-IDF of the vocabularies from the cleaned reviews \n",
    "df_TF_IDF, missed_tfidfW = Preprocessing.words_frequency(cleaned_reviews, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55223f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "The percentage of the covered words in the pretrained dictionary:  0.8740046287216099\n",
      "number of not found words is:  3212\n"
     ]
    }
   ],
   "source": [
    "#Here we construct the embedding matrix of the fetched vocabulary from the reviews. \n",
    "#Each row of the matrix represents the corresponding word embedding vector\n",
    "print(\"Embedding\")\n",
    "embedding_matrix = Preprocessing.create_embedding_matrix(filepath, vocabulary, embedding_dim )\n",
    "\n",
    "#We fetch the list of the words that are not covered by the pretrained dictionary and later eliminate them from the vocabulary list  \n",
    "nonzero_elements, not_foundWords, not_foundIndex = Preprocessing.not_found_words(embedding_matrix, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce0ffe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling make all features(the vectors values) between 0 and 1\n",
    "print(\"Scaling\")\n",
    "X = embedding_matrix\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3d389a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25493, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5b515c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dX = []\n",
    "\n",
    "for i, item in enumerate(X):\n",
    "    annotations = dataset['annotation'][i].split(',')\n",
    "    tail = []\n",
    "    for ann in unique_words:\n",
    "        if ann in annotations: tail.append(1)\n",
    "        else: tail.append(0)\n",
    "    item = np.append(item,tail)\n",
    "    dX.append(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "700afe1a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2603876 , 0.41899434, 0.23521784, 0.43829827, 0.34026442,\n",
       "       0.40032493, 0.56003156, 0.70142562, 0.37277101, 0.51033442,\n",
       "       0.32939404, 0.43210332, 0.45417201, 0.54493978, 0.36291196,\n",
       "       0.45990825, 0.36147312, 0.4431532 , 0.68944668, 0.59450954,\n",
       "       0.37213964, 0.41880861, 0.46740367, 0.48776777, 0.46945583,\n",
       "       0.68560448, 0.68396719, 0.2540808 , 0.54489147, 0.42277035,\n",
       "       0.51028944, 0.52877175, 0.53040765, 0.39871733, 0.61366329,\n",
       "       0.38232154, 0.35523654, 0.30610896, 0.47129632, 0.5349207 ,\n",
       "       0.38760022, 0.59530972, 0.61574073, 0.33836718, 0.68367886,\n",
       "       0.41764642, 0.44392635, 0.53273837, 0.37731185, 0.42808208,\n",
       "       0.39460295, 0.26339785, 0.76145547, 0.42879679, 0.47268829,\n",
       "       0.41896417, 0.50591594, 0.31688774, 0.58912754, 0.44585269,\n",
       "       0.52479806, 0.56856008, 0.78053674, 0.28766522, 0.38274871,\n",
       "       0.64701814, 0.47042689, 0.4194883 , 0.62127616, 0.55399882,\n",
       "       0.48691637, 0.46469735, 0.37194593, 0.33640692, 0.56686061,\n",
       "       0.5879811 , 0.43928429, 0.53999571, 0.26087049, 0.67206752,\n",
       "       0.38972273, 0.53317947, 0.26792516, 0.4811949 , 0.51158745,\n",
       "       0.43721205, 0.58409886, 0.67460722, 0.59550886, 0.51332586,\n",
       "       0.36069604, 0.67758616, 0.49538656, 0.46071234, 0.368191  ,\n",
       "       0.51999907, 0.5008477 , 0.53383897, 0.60122734, 0.58184543,\n",
       "       0.42785238, 0.17624206, 0.52437698, 0.58027172, 0.301666  ,\n",
       "       0.69921228, 0.2651345 , 0.68790324, 0.49158711, 0.70804127,\n",
       "       0.50629925, 0.62806922, 0.52674456, 0.67722315, 0.54678725,\n",
       "       0.43135913, 0.48379248, 0.60961105, 0.45000435, 0.47714619,\n",
       "       0.36336935, 0.24062879, 0.20938358, 0.69137577, 0.34956896,\n",
       "       0.76238575, 0.58929354, 0.47084196, 0.84218599, 0.44769519,\n",
       "       0.38195402, 0.7218126 , 0.44234613, 0.46531086, 0.69543455,\n",
       "       0.37079626, 0.35715051, 0.55497417, 0.50009718, 0.53305742,\n",
       "       0.49062989, 0.54456073, 0.21503371, 0.6699749 , 0.33742407,\n",
       "       0.68015395, 0.50653384, 0.48864985, 0.51912275, 0.53512348,\n",
       "       0.33282827, 0.53311044, 0.64553732, 0.43781371, 0.57176389,\n",
       "       0.49884599, 0.35433918, 0.52798241, 0.32598109, 0.55427191,\n",
       "       0.51914439, 0.27427633, 0.35431129, 0.35344288, 0.56491451,\n",
       "       0.38035709, 0.51258749, 0.41745744, 0.55249421, 0.50351329,\n",
       "       0.54535046, 0.50931396, 0.33542513, 0.6754203 , 0.59597491,\n",
       "       0.40214706, 0.71332888, 0.19914587, 0.70110956, 0.35222901,\n",
       "       0.50187272, 0.59945804, 0.53434912, 0.52444738, 0.46134515,\n",
       "       0.48384055, 0.58110176, 0.47200339, 0.20891705, 0.30654328,\n",
       "       0.25658684, 0.58070703, 0.56106293, 0.43657199, 0.42144973,\n",
       "       0.39138584, 0.60404628, 0.48433765, 0.25420522, 0.57963759,\n",
       "       0.25738277, 0.33660765, 0.5642319 , 0.52828378, 0.3555173 ,\n",
       "       0.57214722, 0.40082267, 0.63769983, 0.48646089, 0.56761029,\n",
       "       0.53272388, 0.45148745, 0.47456766, 0.53955453, 0.51840609,\n",
       "       0.42009574, 0.63388157, 0.37644256, 0.50772486, 0.2288597 ,\n",
       "       0.62198357, 0.51621897, 0.28739964, 0.48974707, 0.46308143,\n",
       "       0.48430488, 0.90424729, 0.54760344, 0.53029555, 0.45367278,\n",
       "       0.44171504, 0.43941352, 0.47993344, 0.50146082, 0.57944135,\n",
       "       0.63419213, 0.45671781, 0.49405285, 0.48979485, 0.54005549,\n",
       "       0.46473164, 0.44837242, 0.4699431 , 0.38521963, 0.56253503,\n",
       "       0.37869294, 0.51884515, 0.49733556, 0.23311299, 0.4748285 ,\n",
       "       0.56545766, 0.47729483, 0.53526548, 0.67799181, 0.34599657,\n",
       "       0.58991114, 0.43701449, 0.44312618, 0.47121144, 0.7119086 ,\n",
       "       0.35108206, 0.47910218, 0.34623025, 0.5348233 , 0.25023145,\n",
       "       0.47095684, 0.44006425, 0.48781449, 0.64751816, 0.48000427,\n",
       "       0.7505883 , 0.48527033, 0.43750852, 0.50564897, 0.56554609,\n",
       "       0.5192858 , 0.57032084, 0.46467841, 0.55685614, 0.62872583,\n",
       "       0.3896708 , 0.52699776, 0.5236946 , 0.57453476, 0.62859282,\n",
       "       0.50910237, 0.5317486 , 0.43803446, 0.61028671, 0.49661372,\n",
       "       0.34509267, 0.56943367, 0.70921847, 0.58833548, 0.4399975 ,\n",
       "       0.38306843, 0.48795145, 0.46101962, 0.60379293, 0.45061874,\n",
       "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX = np.array(dX)\n",
    "dX.shape\n",
    "dX[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc573310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLUSTERING\n",
      "\n",
      "SOM Training\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "##                      CLUSTERING                      ##\n",
    "##########################################################\n",
    "print(\"\\nCLUSTERING\\n\")\n",
    "\n",
    "\n",
    "#define the grid size for the SOM map by g.\n",
    "#@EXTENSION\n",
    "g = 50\n",
    "X_of_map = g\n",
    "y_of_map = g\n",
    "\n",
    "#the input length passed to the SOM is the embedding vectors dimension \n",
    "input_len = X.shape[1]\n",
    "\n",
    "#defining the neghborhood radius sigma as s and the learning rate\n",
    "s = 1; l = 1 # the radius\n",
    "\n",
    "#We create a SOM object and train it on the word vectors that were prepared in the pre-processing step\n",
    "som = MiniSom(x = X_of_map, y= y_of_map, input_len = input_len, sigma = s, learning_rate = l, random_seed = 8)\n",
    "#Randomly initialize the weights that are assigne to each grid cells\n",
    "som.random_weights_init(X)\n",
    "\n",
    "#trining the SOM\n",
    "print(\"SOM Training\")\n",
    "#@EXTENSION\n",
    "som.train_batch(data = X, num_iteration = 1000)\n",
    "\n",
    "\n",
    "\n",
    "# #separating ham and spam reviews from in the dataset\n",
    "# print(\"Seperate\")\n",
    "# ham , spam = Clustering.separate_ham_spam(reviews, y)\n",
    "# #getting vocabulary list that are used in ham and spam reviews\n",
    "\n",
    "# print(\"Cleaning\")\n",
    "# vocabulary_ham, numWords_ham, review_vocab_ham, cleaned_reviews_ham = Preprocessing.cleaning_reviews(ham)\n",
    "# vocabulary_spam, numWords_spam, review_vocab_spam, cleaned_reviews_spam = Preprocessing.cleaning_reviews(spam)\n",
    "# vocabulary_ham = list(set(vocabulary_ham))\n",
    "# vocabulary_spam = list(set(vocabulary_spam))\n",
    "\n",
    "# #creating a list of words that are common among the ham and spam reviews\n",
    "# matches = list(set(vocabulary_spam) & set(vocabulary_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78b71484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Win Map\n"
     ]
    }
   ],
   "source": [
    "#fetching the winning nodes of the word vectors from the SOM grid map\n",
    "print(\"Calculate Win Map\")\n",
    "mappings = som.win_map(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d39c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLASSIFICATION\n",
      "\n",
      "Create Density Image\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "##                      CLASSIFICATION                      ##\n",
    "##############################################################\n",
    "print(\"\\nCLASSIFICATION\\n\")\n",
    "\n",
    "#mappings = som.win_map(X)\n",
    "map_cells = Classify.words_win_cel(mappings, vocabulary) \n",
    "\n",
    "#constructing the review images based on the feature word density\n",
    "print(\"Create Density Image\")\n",
    "dense_img = Classify.create_dens_img(review_vocab, map_cells, vocabulary, not_foundWords, X_of_map, y_of_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing the review images based on the feature word frequency \n",
    "print(\"Create Frequency Image\")\n",
    "frequency_img= Classify.create_freq_img(review_vocab, map_cells, vocabulary, missed_tfidfW, not_foundWords, X_of_map, y_of_map, df_TF_IDF)    \n",
    "\n",
    "\n",
    "\n",
    "#reshaping the images to make them ready for the CNN\n",
    "review_images = Classify.reshape_img(dense_img, frequency_img) \n",
    "\n",
    "#splitting the data to training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(review_images ,label, test_size=0.2,random_state= 252)\n",
    "print(\"Create Model\")\n",
    "model = Classify.create_model(g, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the evaluation performance of the model\n",
    "loss, tr_accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(tr_accuracy)) # \n",
    "loss, te_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(te_accuracy)) # \n",
    "print('\\n')\n",
    "\n",
    "pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "predict_classes = np.argmax(pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis = 1)\n",
    "#print the classification report\n",
    "evaluation = classification_report(expected_classes, predict_classes)\n",
    "print(evaluation)\n",
    "\n",
    "#plotting ROC value of the result\n",
    "matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "ROC_value, fprx, tpry = Classify.ROC_Val(matrix)\n",
    "\n",
    "globalTime = time.perf_counter() - globalTime\n",
    "print(\"Total Time: \", globalTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
