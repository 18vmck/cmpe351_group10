{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe457656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simon Dudtschak\n",
    "# 20166103\n",
    "#\n",
    "# Based on: SOM-CNN by Ashraf Neisari\n",
    "# Available: https://github.com/Ashsari/spam_review_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68763433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from preprocessing import Preprocessing\n",
    "from Clustering_words_SOM import Clustering\n",
    "from Constructing_images_classification import Classify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from SRD_minisom import MiniSom\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import collections\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265a57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.randint(0,10000)\n",
    "np.random.seed(rand)\n",
    "random.seed(rand)\n",
    "\n",
    "globalTime = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b6c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRE-PROCESSING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "##                      PRE-PROCESSING                      ##\n",
    "##############################################################\n",
    "print(\"\\nPRE-PROCESSING\\n\")\n",
    "\n",
    "#importing the dataset\n",
    "#@EXTENSION\n",
    "dataset = pd.read_csv('final_mfrc_data.csv')\n",
    "reviews = dataset.iloc[:,0]\n",
    "\n",
    "#get labels and convert them to categorical format\n",
    "# list_words = []\n",
    "# for row in dataset[\"annotation\"]:\n",
    "#     words = row.split(',')\n",
    "#     word_counts = collections.Counter(words)\n",
    "#     list_words.append(words)\n",
    "#     flat_list = list(chain.from_iterable(list_words))\n",
    "# unique_words = set(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f90117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels and convert them to categorical format\n",
    "y = dataset.iloc[:,3]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "label = to_categorical(y, num_classes = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad0137e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning\n"
     ]
    }
   ],
   "source": [
    "#cleaning the vocabulary and tokenizing the reviews\n",
    "print(\"Cleaning\")\n",
    "vocabulary, numWords, review_vocab, cleaned_reviews = Preprocessing.cleaning_reviews(reviews)\n",
    "\n",
    "# keeping only unique words     \n",
    "vocabulary = list(set(vocabulary))\n",
    "\n",
    "# Here we provide the pretrained word embedding dictionary of glove with the vector dimension of 300\n",
    "embedding_dim = 300\n",
    "filepath ='glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7237174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\simon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#calculate the TF-IDF of the vocabularies from the cleaned reviews \n",
    "df_TF_IDF, missed_tfidfW = Preprocessing.words_frequency(cleaned_reviews, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d61979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "The percentage of the covered words in the pretrained dictionary:  0.8740046287216099\n",
      "number of not found words is:  3212\n"
     ]
    }
   ],
   "source": [
    "#Here we construct the embedding matrix of the fetched vocabulary from the reviews. \n",
    "#Each row of the matrix represents the corresponding word embedding vector\n",
    "print(\"Embedding\")\n",
    "embedding_matrix = Preprocessing.create_embedding_matrix(filepath, vocabulary, embedding_dim )\n",
    "\n",
    "#We fetch the list of the words that are not covered by the pretrained dictionary and later eliminate them from the vocabulary list  \n",
    "nonzero_elements, not_foundWords, not_foundIndex = Preprocessing.not_found_words(embedding_matrix, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a4c4b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling make all features(the vectors values) between 0 and 1\n",
    "print(\"Scaling\")\n",
    "X = embedding_matrix\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0554a019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25493, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507f1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d958cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLUSTERING\n",
      "\n",
      "SOM Training\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "##                      CLUSTERING                      ##\n",
    "##########################################################\n",
    "print(\"\\nCLUSTERING\\n\")\n",
    "\n",
    "\n",
    "#define the grid size for the SOM map by g.\n",
    "#@EXTENSION\n",
    "g = 50\n",
    "X_of_map = g\n",
    "y_of_map = g\n",
    "\n",
    "#the input length passed to the SOM is the embedding vectors dimension \n",
    "input_len = X.shape[1]\n",
    "\n",
    "#defining the neghborhood radius sigma as s and the learning rate\n",
    "s = 1; l = 1 # the radius\n",
    "\n",
    "#We create a SOM object and train it on the word vectors that were prepared in the pre-processing step\n",
    "som = MiniSom(x = X_of_map, y= y_of_map, input_len = input_len, sigma = s, learning_rate = l, random_seed = 8)\n",
    "#Randomly initialize the weights that are assigne to each grid cells\n",
    "som.random_weights_init(X)\n",
    "\n",
    "#trining the SOM\n",
    "print(\"SOM Training\")\n",
    "#@EXTENSION\n",
    "som.train_batch(data = X, num_iteration = 1000)\n",
    "\n",
    "\n",
    "\n",
    "#separating ham and spam reviews from in the dataset\n",
    "print(\"Seperate\")\n",
    "ham , spam =Clustering.separate_ham_spam(reviews, y)\n",
    "#getting vocabulary list that are used in ham and spam reviews\n",
    "\n",
    "print(\"Cleaning\")\n",
    "vocabulary_ham, numWords_ham, review_vocab_ham, cleaned_reviews_ham = Preprocessing.cleaning_reviews(ham)\n",
    "vocabulary_spam, numWords_spam, review_vocab_spam, cleaned_reviews_spam = Preprocessing.cleaning_reviews(spam)\n",
    "vocabulary_ham = list(set(vocabulary_ham))\n",
    "vocabulary_spam = list(set(vocabulary_spam))\n",
    "\n",
    "#creating a list of words that are common among the ham and spam reviews\n",
    "matches = list(set(vocabulary_spam) & set(vocabulary_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34862706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the winning nodes of the word vectors from the SOM grid map\n",
    "print(\"Calculate Win Map\")\n",
    "mappings = som.win_map(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "##                      CLASSIFICATION                      ##\n",
    "##############################################################\n",
    "print(\"\\nCLASSIFICATION\\n\")\n",
    "\n",
    "#mappings = som.win_map(X)\n",
    "map_cells = Classify.words_win_cel(mappings, vocabulary) \n",
    "\n",
    "#constructing the review images based on the feature word density\n",
    "print(\"Create Density Image\")\n",
    "dense_img = Classify.create_dens_img(review_vocab, map_cells, vocabulary, not_foundWords, X_of_map, y_of_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing the review images based on the feature word frequency \n",
    "print(\"Create Frequency Image\")\n",
    "frequency_img= Classify.create_freq_img(review_vocab, map_cells, vocabulary, missed_tfidfW, not_foundWords, X_of_map, y_of_map, df_TF_IDF)    \n",
    "\n",
    "\n",
    "\n",
    "#reshaping the images to make them ready for the CNN\n",
    "review_images = Classify.reshape_img(dense_img, frequency_img) \n",
    "\n",
    "#splitting the data to training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(review_images ,label, test_size=0.2,random_state= 252)\n",
    "print(\"Create Model\")\n",
    "model = Classify.create_model(g, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8159b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the evaluation performance of the model\n",
    "loss, tr_accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(tr_accuracy)) # \n",
    "loss, te_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(te_accuracy)) # \n",
    "print('\\n')\n",
    "\n",
    "pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "predict_classes = np.argmax(pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis = 1)\n",
    "#print the classification report\n",
    "evaluation = classification_report(expected_classes, predict_classes)\n",
    "print(evaluation)\n",
    "\n",
    "#plotting ROC value of the result\n",
    "matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "ROC_value, fprx, tpry = Classify.ROC_Val(matrix)\n",
    "\n",
    "globalTime = time.perf_counter() - globalTime\n",
    "print(\"Total Time: \", globalTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
